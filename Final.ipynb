{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7aeb45b9f0e242758f3adaba073f1aca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b78bb2bb916c45d5a895e4459b9ed2c5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e4dd8d57dfb0416b8b23fd2e63988adf",
              "IPY_MODEL_d16dacf6227b43138f431d83ca92d1b6"
            ]
          }
        },
        "b78bb2bb916c45d5a895e4459b9ed2c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e4dd8d57dfb0416b8b23fd2e63988adf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6951ca9081414c1b9b78059402f6b84b",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 20,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 20,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cfc403f9787b43beb7a2dd282478ef78"
          }
        },
        "d16dacf6227b43138f431d83ca92d1b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6ad5f5b37160487aab7f809c347e31d8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 20/20 [00:22&lt;00:00,  1.14s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a32217008aeb4a7c8259d6b61e1e968f"
          }
        },
        "6951ca9081414c1b9b78059402f6b84b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cfc403f9787b43beb7a2dd282478ef78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6ad5f5b37160487aab7f809c347e31d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a32217008aeb4a7c8259d6b61e1e968f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f5781a293e374fa5ad05374c4a157f2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a4784e0664fc49f6b049cbb376f9f865",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_58e5f9c9ecc84eff85bbe5c617d041e9",
              "IPY_MODEL_8dd8d9fadf164af8825ddd99bf5aca44"
            ]
          }
        },
        "a4784e0664fc49f6b049cbb376f9f865": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "58e5f9c9ecc84eff85bbe5c617d041e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cd5cb4826b6049e4bbb868665796640c",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 20,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 20,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9a6cdba420474643a7d6975951bbccc8"
          }
        },
        "8dd8d9fadf164af8825ddd99bf5aca44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1a73118396be43eb994f5df79834af00",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 20/20 [00:00&lt;00:00, 105.63it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0b4fc514a2b944309fb8455748fefd84"
          }
        },
        "cd5cb4826b6049e4bbb868665796640c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9a6cdba420474643a7d6975951bbccc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1a73118396be43eb994f5df79834af00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0b4fc514a2b944309fb8455748fefd84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTzle9XEuWR_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "a8b6629d-aee4-4475-bf74-28906dff2eea"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt3vnEsWHszq",
        "colab_type": "text"
      },
      "source": [
        "Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjp_kcB0wdI-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "591adda8-57d0-4ace-cb2a-2644d57bc745"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gzip\n",
        "import pickle\n",
        "\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "import re\n",
        "import datetime\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "%load_ext tensorboard\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import FastText\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.layers import Input, Softmax, RNN, Dense, Embedding, GRU,LSTM,concatenate,Flatten,GlobalMaxPooling1D,GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "import nltk.translate.bleu_score as bleu\n",
        "\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "tf.executing_eagerly()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIFPYLjRHxkF",
        "colab_type": "text"
      },
      "source": [
        "Fixing Random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCWkKqk_wgrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random as rn\n",
        "\n",
        "##fixing numpy RS\n",
        "np.random.seed(1)\n",
        "\n",
        "##fixing tensorflow RS\n",
        "tf.random.set_seed(2)\n",
        "#tf.set_random_seed(2)\n",
        "\n",
        "##python RS\n",
        "rn.seed(3)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlFsf8UTTiq-",
        "colab_type": "text"
      },
      "source": [
        "## Seq2Seq with Attention Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjM32k00cMER",
        "colab_type": "text"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf25Y1b5Th9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size,text_vocab_sizes, embedding_dim, input_length,text_features_len, enc_units ,emb_weights,text_features_emb_weights,emb_trainable):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.input_length = input_length\n",
        "        self.text_features_len = text_features_len\n",
        "        self.enc_units= enc_units\n",
        "        self.gru_output = 0\n",
        "        self.state_h=0\n",
        "        self.state_c=0\n",
        "        self.text_vocab_sizes = text_vocab_sizes\n",
        "        self.emb_weights = emb_weights\n",
        "        self.text_emb_weights = text_features_emb_weights\n",
        "        self.emb_trainable = emb_trainable\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length,\n",
        "                                   mask_zero=True,weights = self.emb_weights,trainable = self.emb_trainable,\n",
        "                                   name=\"embedding_layer_question\")\n",
        "        \n",
        "        self.embedding1 = Embedding(input_dim=self.text_vocab_sizes[0], output_dim=self.embedding_dim, input_length=self.text_features_len[0],\n",
        "                                   weights = self.text_emb_weights[0],trainable = self.emb_trainable,name=\"embedding_layer_textfeature1\")\n",
        "        \n",
        "        self.embedding2 = Embedding(input_dim=self.text_vocab_sizes[1], output_dim=self.embedding_dim, input_length=self.text_features_len[1],\n",
        "                                   weights = self.text_emb_weights[1],trainable = self.emb_trainable,name=\"embedding_layer_textfeature2\")\n",
        "\n",
        "        self.embedding3 = Embedding(input_dim=self.text_vocab_sizes[2], output_dim=self.embedding_dim, input_length=self.text_features_len[2],\n",
        "                                   weights = self.text_emb_weights[2],trainable = self.emb_trainable,name=\"embedding_layer_textfeature3\")\n",
        "        \n",
        "        self.embedding4 = Embedding(input_dim=self.text_vocab_sizes[3], output_dim=self.embedding_dim, input_length=self.text_features_len[3],\n",
        "                                   weights = self.text_emb_weights[3],trainable = self.emb_trainable,name=\"embedding_layer_textfeature4\")\n",
        "        \n",
        "        self.gru = GRU(self.enc_units, return_state=True, return_sequences=True, name=\"Encoder_GRU_question\")\n",
        "\n",
        "        self.maxpool = GlobalMaxPooling1D()\n",
        "        self.averagepool  = GlobalAveragePooling1D()\n",
        "        self.dense1 = Dense(self.enc_units)\n",
        "        self.dense2 = Dense(self.enc_units)\n",
        "        \n",
        "    def call(self, input ,text_features ,other_features ,mask):\n",
        "\n",
        "        #print(\"=\"*20, \"ENCODER\", \"=\"*20,\"\\n\")\n",
        "        #print(\"ENCODER ==> INPUT SEQUENCES SHAPE :\",input_sentences.shape)\n",
        "        try:\n",
        "            input = self.embedding(input)\n",
        "            mask = input._keras_mask\n",
        "            text_features[0] = self.embedding1(text_features[0])\n",
        "            text_features[1] = self.embedding2(text_features[1])\n",
        "            text_features[2] = self.embedding3(text_features[2])\n",
        "            text_features[3] = self.embedding4(text_features[3])\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "\n",
        "        #print(\"ENCODER ==>MASK SHAPE\",mask.shape)\n",
        "\n",
        "\n",
        "        #print(\"ENCODER ==> AFTER EMBEDDING THE INPUT SHAPE :\",input_embedd.shape)\n",
        "\n",
        "        #self.lstm_output, self.state_h,self.state_c = self.lstm(input_embedd,mask=mask)\n",
        "        self.gru_output, self.state_h = self.gru(input,mask=mask)\n",
        "        self.state_c = None\n",
        "\n",
        "        #print(\"ENCODER  ==> GRU HIDDEN STATE SHAPE\",self.state_h.shape)\n",
        "        #print(\"ENCODER  ==> GRU OUTPUT SHAPE\",self.lstm_output.shape)\n",
        "\n",
        "\n",
        "        #print(\"embedd1 \",embedd1.shape)\n",
        "        output11 = self.maxpool(text_features[0])\n",
        "        output12  = self.averagepool(text_features[0])\n",
        "        output1 = concatenate([output11,output12])\n",
        "        #print(\"output1 \",output1.shape)\n",
        "\n",
        "        #print(\"embedd2 \",embedd2.shape)\n",
        "        output21= self.maxpool(text_features[1])\n",
        "        output22  = self.averagepool(text_features[1])\n",
        "        output2 = concatenate([output21,output22])\n",
        "        #print(\"output2 \",output2.shape)\n",
        "\n",
        "        #print(\"embedd3 \",embedd3.shape)\n",
        "        output31= self.maxpool(text_features[2])\n",
        "        output32  = self.averagepool(text_features[2])\n",
        "        output3 = concatenate([output31,output32])\n",
        "        #print(\"output3 \",output3.shape)\n",
        " \n",
        "\n",
        "        #print(\"embedd4 \",embedd4.shape)\n",
        "        output41= self.maxpool(text_features[3])\n",
        "        output42  = self.averagepool(text_features[3])\n",
        "        output4 = concatenate([output41,output42])\n",
        "        #print(\"output4 \",output4.shape)\n",
        "        \n",
        "\n",
        "        output5 = self.dense1(other_features)\n",
        "        #print(\"dense \",output5.shape)\n",
        "        \n",
        "        #Concatenating all feature outputs and question lstm output.\n",
        "        concat = concatenate([output1, output2,output3,output4,output5,self.state_h],axis=1)\n",
        "        #print(\"concat_layer\",concat.shape)\n",
        "\n",
        "        self.state_h = self.dense2(concat)\n",
        "        #print(\"dense \",self.state_h)'''\n",
        "\n",
        "\n",
        "        #print(\"-\"*27)\n",
        "        #print(\"ENCODER ==> OUTPUT SHAPE\",self.lstm_output.shape)\n",
        "        #print(\"ENCODER ==> HIDDEN STATE SHAPE\",self.state_h.shape)\n",
        "        #print(\"ENCODER ==> CELL STATE SHAPE\", self.state_c.shape)\n",
        "\n",
        "        return self.gru_output,self.state_h,self.state_c\n",
        "        \n",
        "    def get_states(self):\n",
        "        return self.state_h,self.state_c"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HSvDgb8cPow",
        "colab_type": "text"
      },
      "source": [
        "### Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NapuI7IcIbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self, score_type,att_units):\n",
        "        super(Attention, self).__init__()\n",
        "        self.score_type=score_type\n",
        "        self.att_units=att_units\n",
        "        \n",
        "    def build(self,input_shape):\n",
        "        if self.score_type=='dot':\n",
        "            pass\n",
        "        \n",
        "        elif self.score_type=='general':\n",
        "            self.dense = Dense(self.att_units)\n",
        "            \n",
        "        elif self.score_type=='concat':\n",
        "            self.dense_1 = Dense(self.att_units)\n",
        "            self.dense_2 = Dense(self.att_units)\n",
        "            self.output_layer = Dense(1)\n",
        "            \n",
        "        \n",
        "    def call(self, state_h, encoder_output):\n",
        "\n",
        "        # state shape == (batch_size, hidden size)\n",
        "        # state_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # context_vector shape == (batch_size, max_len, hidden size)\n",
        "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "        state_with_time_axis = tf.expand_dims(state_h, 1)\n",
        "\n",
        "        if self.score_type=='dot':\n",
        "            score=tf.matmul(encoder_output,state_with_time_axis, transpose_b=True)\n",
        "            #print(\"ATTENTION ==> DOT_SCORE SHAPE\",score.shape)\n",
        "\n",
        "        elif self.score_type=='general':\n",
        "            layer = self.dense (encoder_output)\n",
        "            score = tf.matmul (layer,state_with_time_axis, transpose_b=True)\n",
        "            #print(\"ATTENTION ==> GENERAL_SCORE SHAPE\",score.shape)\n",
        "\n",
        "        elif self.score_type=='concat':\n",
        "            # score shape == (batch_size, max_length, 1)\n",
        "            # we get 1 at the last axis because we are applying score to self.output_layer\n",
        "            # the shape of the tensor before applying self.output_layer is (batch_size, max_length, units)\n",
        "            layer_1 = self.dense_1 (state_with_time_axis)\n",
        "            layer_2 = self.dense_2 (encoder_output)\n",
        "            concat = layer_1 + layer_2\n",
        "            score=self.output_layer(tf.nn.tanh(concat)) \n",
        "            #print(\"ATTENTION ==> CONCAT_SCORE SHAPE\",score.shape)\n",
        "\n",
        "    \n",
        "        #attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        \n",
        "        #context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * encoder_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQTr0bZmcRiT",
        "colab_type": "text"
      },
      "source": [
        "### One-Step Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfYjRzYIanO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class One_Step_Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, input_length, dec_units ,score_type ,att_units,emb_weights,emb_trainable):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.emb_weights = emb_weights\n",
        "        self.emb_trainable = emb_trainable\n",
        "        self.dec_units = dec_units\n",
        "        self.input_length = input_length\n",
        "        self.score_type= score_type\n",
        "        self.att_units= att_units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=input_shape,\n",
        "                                   mask_zero=True,weights = self.emb_weights,trainable = self.emb_trainable ,name=\"embedding_layer_decoder\")\n",
        "        \n",
        "        self.gru = GRU(self.dec_units, return_sequences=True, return_state=True, name=\"Decoder_LSTM\")\n",
        "\n",
        "        self.attention=Attention(self.score_type,self.att_units)\n",
        "\n",
        "        self.dense   = Dense(self.vocab_size, activation='softmax')\n",
        "\n",
        "        \n",
        "    def call(self, target_sentences , encoder_output, state_h, state_c ):\n",
        "        \n",
        "        context_vector , attention_weights = self.attention (state_h, encoder_output)\n",
        "\n",
        "        try:\n",
        "            target_sentences = self.embedding(target_sentences)\n",
        "            mask = target_sentences._keras_mask\n",
        "        except:\n",
        "            pass\n",
        "        dec_input = tf.concat([tf.expand_dims(context_vector, 1), target_sentences], axis=-1)\n",
        "\n",
        "        #lstm_output, state_h, state_c = self.lstm(dec_input,mask=mask, initial_state=[state_h, state_c])\n",
        "        gru_output, state_h= self.gru(dec_input,mask=mask, initial_state=[state_h])\n",
        "        state_c = None\n",
        "\n",
        "        gru_output = tf.reshape(gru_output, (-1, gru_output.shape[2]))\n",
        "        output = self.dense(gru_output)\n",
        "\n",
        "        return output, state_h, state_c, attention_weights, context_vector"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6lAPYiIcV5J",
        "colab_type": "text"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wInHpGwVdqkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, input_length, dec_units ,score_type ,att_units ,emb_weights,emb_trainable):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.onestepdecoder = One_Step_Decoder(vocab_size, embedding_dim, input_length, \n",
        "                                               dec_units ,score_type ,att_units,emb_weights,emb_trainable)        \n",
        "        #self.targ_lang=tokenizer\n",
        "\n",
        "        \n",
        "    def call(self, target_sentences,encoder_output, state_h, state_c  ):\n",
        "\n",
        "        #print(\"\\n\",\"=\"*20, \"DECODER\", \"=\"*20,\"\\n\")\n",
        "        #print(\"DECODER ==> INPUT SEQUENCES SHAPE :\",target_sentences.shape)\n",
        "        #print(\"WE ARE INITIALIZING DECODER WITH ENCODER STATES :\",state_h.shape, state_c.shape)\n",
        "        \n",
        "        #print(\"\\n\",\"=\"*20, \"ATTENTION\", \"=\"*20,\"\\n\")\n",
        "        #print(\"ATTENTION ==> INPUT SEQUENCES SHAPE :\",encoder_output.shape)\n",
        "        #print(\"ATTENTION ==> INPUT HIDDEN STATE SHAPE\",state_h.shape)\n",
        "        #print(\"-\"*27)\n",
        "\n",
        "        all_outputs=tf.TensorArray(tf.float32,size=target_sentences.shape[1], name='output_array')\n",
        "       # dec_input = target_sentences[:,0:1]\n",
        "        dec_input=tf.expand_dims(target_sentences[:,0], 1)\n",
        "        for timestep in range(1,target_sentences.shape[1]):\n",
        "\n",
        "            output,state_h, state_c, attention_weights ,context_vector = self.onestepdecoder(dec_input,\n",
        "                                                                            encoder_output,state_h, state_c)\n",
        "\n",
        "            dec_input = tf.expand_dims(target_sentences[:,timestep], 1)\n",
        "\n",
        "            all_outputs=all_outputs.write(timestep,output)\n",
        "        \n",
        "        all_outputs=tf.transpose(all_outputs.stack(), [1,0,2])\n",
        "\n",
        "        #print(\"ATTENTION ==> ATTENTION_WEIGHTS SHAPE\", attention_weights.shape)\n",
        "        #print(\"ATTENTION ==> OUTPUT SHAPE\",context_vector.shape)\n",
        "        #print(\"-\"*27)\n",
        "\n",
        "        #print(\"\\nFINAL OUTPUT SHAPE\",all_outputs.shape)\n",
        "        #print(\"=\"*50)\n",
        "\n",
        "        return all_outputs\n",
        "        \n",
        "    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulTeXpYM78ef",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nr26X1RUQLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(Model):\n",
        "    def __init__(self, seq_len,text_features_len, vocab_sizes,text_vocab_sizes,score_type , units ,emb_weights=[None,None,None,None,None,None],emb_trainable=True):\n",
        "        super().__init__() # https://stackoverflow.com/a/27134600/4084039\n",
        "        \n",
        "        self.encoder =Encoder( vocab_sizes[0],text_vocab_sizes, 150, seq_len[0],text_features_len, units[0],emb_weights[0],emb_weights[2:],emb_trainable)\n",
        "        \n",
        "        self.decoder = Decoder(vocab_sizes[1], 150, seq_len[1], units[1],\n",
        "                              score_type , units[2], emb_weights[1],emb_trainable)\n",
        "\n",
        "    def call(self,data):\n",
        "        questions,answers = data[0],data[1]\n",
        "        text_features = [data[2],data[3],data[4],data[5]]\n",
        "        other_features = data[6]\n",
        "        ques_mask  = data[7]\n",
        "        encoder_output , encoder_h, encoder_c = self.encoder(questions,text_features,other_features,ques_mask)\n",
        "\n",
        "        #print(\"=\"*20, \"ENCODER\", \"=\"*20)\n",
        "        #print(\"-\"*27)\n",
        "        #print(\"ENCODER ==> OUTPUT SHAPE\",encoder_output.shape)\n",
        "        #print(\"ENCODER ==> HIDDEN STATE SHAPE\",encoder_h.shape)\n",
        "        #print(\"ENCODER ==> CELL STATE SHAPE\", encoder_c.shape)\n",
        "        #print(\"=\"*20, \"DECODER\", \"=\"*20)\n",
        "\n",
        "        decoder_output = self.decoder(answers,encoder_output, encoder_h, encoder_c )\n",
        "\n",
        "        #print(\"-\"*27)\n",
        "        #print(\"FINAL OUTPUT SHAPE\",decoder_output.shape)\n",
        "        #print(\"=\"*50)\n",
        "\n",
        "        return decoder_output\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYgo6unEjufx",
        "colab_type": "text"
      },
      "source": [
        "### Masked Loss Function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5pABdcrju5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reference https://udibhaskar.github.io/practical-ml/debugging%20nn/neural%20network/overfit/underfit/2020/02/03/Effective_Training_and_Debugging_of_a_Neural_Networks.html\n",
        "\n",
        "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "def maskedLoss(y_true, y_pred):\n",
        "\n",
        "    #getting mask value\n",
        "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
        "    \n",
        "    #calculating the loss\n",
        "    loss_ = loss_function (y_true, y_pred)\n",
        "    print()\n",
        "    #converting mask dtype to loss_ dtype\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    \n",
        "    #applying the mask to loss\n",
        "    loss_ = loss_*mask\n",
        "    \n",
        "    #getting mean over all the values\n",
        "    loss_ = tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "    return loss_\n",
        "    "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdJ7TEwcrqNh",
        "colab_type": "text"
      },
      "source": [
        "## Pre-Processing Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw0SUBx-yOLN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_tokens(sentence):\n",
        "    ''' Removs <start> and <end> token if present '''\n",
        "    \n",
        "    sentence = sentence.replace('<start>','')\n",
        "    sentence = sentence.replace('<end>','')\n",
        "\n",
        "    return sentence"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PfxIB7ErrWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decontraction(text):\n",
        "    ''' De-contract words in sentences '''\n",
        "    \n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
        "    text = re.sub(\"ain\\'t\", \"are not\" , text)\n",
        "    \n",
        "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'s\", \" is\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'m\", \" am\", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocess_sentence(sentence , max_len , model ,add_tokens=True):\n",
        "\n",
        "    #Removes start and end token if already present\n",
        "    sentence = remove_tokens(sentence)\n",
        "\n",
        "    #Lower case conversion\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    #De-contracting words\n",
        "    sentence = decontraction(sentence)\n",
        "\n",
        "    #Adding a space between a word and the punctuation\n",
        "    sentence = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    \n",
        "    #Removing Special Characters\n",
        "    sentence = re.sub(r\"[^a-zA-Z0-9?.!,Â¿]+\", \" \", sentence)\n",
        "\n",
        "    #Removing Extra Spaces\n",
        "    sentence = re.sub(' +',' ',sentence)\n",
        "\n",
        "    #Adding Start and End tokens\n",
        "    if add_tokens == True:\n",
        "        sentence = '<start> ' + sentence + ' <end>'\n",
        "\n",
        "    # Masking and Padding data \n",
        "    sentence = sentence.split()\n",
        "    sentence  = sentence[:max_len]\n",
        "    n = len(sentence)\n",
        "    mask = [1]*len(sentence)\n",
        "    pad = [0]*( max_len- n)\n",
        "    sentence.extend(pad)\n",
        "    mask.extend([0]*len(pad))\n",
        "\n",
        "    #Create a weight matrix for words \n",
        "    embedding_weights = np.zeros((max_len, 150))\n",
        "    for i in range(n):\n",
        "        try:\n",
        "            embedding_vector = model[sentence[i]]\n",
        "            embedding_weights[i] = embedding_vector\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    mask = np.array(mask)\n",
        "    mask = tf.not_equal(mask,0)\n",
        "\n",
        "    return embedding_weights,mask"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVdpLziBRxgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_answer(sentence , max_len ,tokenizer, model ,add_tokens=True):\n",
        "\n",
        "    #Removes start and end token if already present\n",
        "    sentence = remove_tokens(sentence)\n",
        "\n",
        "    #Lower case conversion\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    #De-contracting words\n",
        "    sentence = decontraction(sentence)\n",
        "\n",
        "    #Adding a space between a word and the punctuation\n",
        "    sentence = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    \n",
        "    #Removing Special Characters\n",
        "    sentence = re.sub(r\"[^a-zA-Z0-9?.!,Â¿]+\", \" \", sentence)\n",
        "\n",
        "    #Removing Extra Spaces\n",
        "    sentence = re.sub(' +',' ',sentence)\n",
        "\n",
        "    #Adding Start and End tokens\n",
        "    if add_tokens == True:\n",
        "        sentence = '<start> ' + sentence + ' <end>'\n",
        "\n",
        "    #Tokenizing\n",
        "    sentence  = tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "    #Padding\n",
        "    sentence = tf.keras.preprocessing.sequence.pad_sequences([sentence], maxlen=max_len,padding='post')[0]\n",
        "\n",
        "    return sentence"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Tz1S51S262-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_other_features(other_features,encoders):\n",
        "    for i in range(4):\n",
        "        other_features[i] = encoders[i].transform([other_features[i]])[0]\n",
        "\n",
        "    price = other_features[4]\n",
        "    if not type(price)==float:\n",
        "        if not len(price.split())==1:\n",
        "            price = price.split('-')[0]\n",
        "        price = price.replace('$','')\n",
        "        other_features[4] = float(price.replace(',',''))\n",
        "\n",
        "    other_features=other_features.astype(np.float32)\n",
        "\n",
        "    return other_features\n",
        "    "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7boEDDw8tdUI",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqEQyjjniUtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_perplexity(loss):\n",
        "    return tf.exp(loss).numpy()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CePnMWTtdtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(question ,text_features ,other_features,encoders,tokenizers,seq_len,text_features_seqlen,fastext_models,model):\n",
        "    \n",
        "    ques_tokenizer=tokenizers[0]\n",
        "    ans_tokenizer=tokenizers[1]\n",
        "\n",
        "    ques_seqlen=seq_len[0]\n",
        "    ans_seqlen=seq_len[1]\n",
        "    sentence = question\n",
        "\n",
        "    #Pre-processing data\n",
        "    question,mask = preprocess_sentence(question,ques_seqlen,fastext_models[0])\n",
        "    for i in range(len(text_features)):\n",
        "        text_features[i],_ = preprocess_sentence(text_features[i],text_features_seqlen[i],fastext_models[i+2],add_tokens=False)\n",
        "    other_features = process_other_features(other_features,encoders)\n",
        "\n",
        "    question = np.expand_dims(question,0)\n",
        "    \n",
        "    mask = tf.expand_dims(mask,0)\n",
        "\n",
        "    for i in range(len(text_features)):\n",
        "        text_features[i]  =  np.expand_dims(text_features[i],0)\n",
        "    other_features = np.expand_dims(other_features,0)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    #Fetching encoder output\n",
        "    enc_out , state_h, state_c = model.layers[0]( question ,text_features ,other_features ,mask )\n",
        "\n",
        "    #Initializing decoder input with <start> token\n",
        "    dec_input = tf.expand_dims([ans_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    #Reversing Word,Index Dictionary (i.e swaping keys and values)\n",
        "    word_dict = dict(zip(ans_tokenizer.word_index.values(),ans_tokenizer.word_index.keys()))\n",
        "\n",
        "    for i in range(ans_seqlen):\n",
        "\n",
        "        predictions, state_h, state_c, attention_weights,_ = model.layers[1].onestepdecoder(dec_input,enc_out, state_h, state_c )\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        if word_dict.get(predicted_id,'None') == '<end>':\n",
        "            return result\n",
        "            \n",
        "        result += word_dict.get(predicted_id,'None') + ' '\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        \n",
        "    return result"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HuoBN-St3VJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import random \n",
        "def predict_answers(X,encoders,tokenizers,seq_lengths,fastext_models,model,verbose=0): \n",
        "\n",
        "    ''' Predicts replies using given model and data(X,y,encoders,tokenizers,seq_lengths)\n",
        "        Returns: Questions and Predicted Answers '''\n",
        "\n",
        "    text_columns = ['description','title','feature','brand']\n",
        "\n",
        "    seq_len = [seq_lengths[0],seq_lengths[1]]\n",
        "    text_features_seqlen = [seq_lengths[2],seq_lengths[3],seq_lengths[4],seq_lengths[5]]\n",
        "\n",
        "    pred_answers = []\n",
        "    if verbose !=0:\n",
        "        print('Predicting Answers ...')\n",
        "    for i in tqdm(range(len(X))):\n",
        "\n",
        "        #Fetching Question\n",
        "        question = X['question'].values[i]\n",
        "\n",
        "        #Fetching text features\n",
        "        text_features = []\n",
        "        for column in text_columns:\n",
        "            text_features.append(X[column].values[i])\n",
        "\n",
        "        #Fetching other features\n",
        "        other_features = (X[['questionType','answerType','category','main_cat','price']].values)[i]\n",
        "\n",
        "        #Predicted Answer\n",
        "        pred_answer = evaluate(question ,text_features ,other_features,encoders,tokenizers,seq_len,text_features_seqlen,fastext_models, model)\n",
        "        pred_answers.append(pred_answer)\n",
        "\n",
        "        if verbose ==2:\n",
        "            print(\"Question -->\",remove_tokens(question))\n",
        "            print(\"Predicted Answer --> \",pred_answer,'\\n')\n",
        "            print(\"=\"*80,'\\n')\n",
        "\n",
        "\n",
        "    return pred_answers"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M85li64GYh4k",
        "colab_type": "text"
      },
      "source": [
        "# Function 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAgK7q20bklJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Function_1(X,y=np.nan,verbose=0):\n",
        "    ''' Pre-processes and predicts answers for the given input\n",
        "    X --> Input Data (Dataframe or numpy array)\n",
        "    y --> True answers (if provided then returns the metric)\n",
        "    verbose --> 1 --> prints the basic progress.\n",
        "                2 --> prints the basic progress, question and their predicted answers.\n",
        "\n",
        "    returns --> predicted_answers if not given.\n",
        "            --> Metric if y is given.\n",
        "    '''\n",
        "    if not 'encoders' in globals():\n",
        "        #Loading Label Encoders\n",
        "        global encoders\n",
        "        with open('/content/drive/My Drive/Attention/encoders.pickle', 'rb') as file:\n",
        "            encoders = pickle.load(file)\n",
        "\n",
        "    if not 'tokenizers' in globals():\n",
        "        global tokenizers\n",
        "        #Loading Tokenizers\n",
        "        with open('/content/drive/My Drive/Attention/tokenizers.pickle', 'rb') as file:\n",
        "            tokenizers = pickle.load(file)\n",
        "\n",
        "    if not 'fastext_models' in globals():\n",
        "        #Loading Fasttext\n",
        "        global fastext_models\n",
        "        with open('/content/drive/My Drive/Attention/fastext_models.pickle', 'rb') as file:\n",
        "            fastext_models = pickle.load(file)\n",
        "\n",
        "            \n",
        "    vocab_sizes=[]\n",
        "    for i in range(2):\n",
        "        vocab_size = len(tokenizers[i].word_index) + 1\n",
        "        vocab_sizes.append(vocab_size) \n",
        "\n",
        "    text_vocab_sizes = []\n",
        "    for i in range(2,6):\n",
        "        vocab_size = len(tokenizers[i].word_index) + 1\n",
        "        text_vocab_sizes.append(vocab_size) \n",
        "\n",
        "    if not 'seq_lengths' in globals():\n",
        "        global seq_lengths\n",
        "        #Loading Sequence Lengths\n",
        "        with open('/content/drive/My Drive/Attention/seq_len.pickle', 'rb') as file:\n",
        "            seq_lengths = pickle.load(file)\n",
        "\n",
        "    if not 'data' in globals():\n",
        "        global data\n",
        "        #Loading Data to initialize model\n",
        "        with open('/content/drive/My Drive/Attention/data.pickle', 'rb') as file:\n",
        "            data = pickle.load( file)\n",
        "        data.append(np.zeros((len(data[0]),1)))\n",
        "\n",
        "    if not 'answers_train' in globals():\n",
        "        global answers_train\n",
        "        #Loading Answers\n",
        "        with open('/content/drive/My Drive/Attention/answers_train.pickle', 'rb') as file:\n",
        "            answers_train = pickle.load( file)\n",
        "\n",
        "\n",
        "    if type(X)== np.ndarray:\n",
        "        if verbose!=0:\n",
        "            print(\"Got Array as an Input ...\")\n",
        "\n",
        "        #Re-shaping if the array has only 1 row\n",
        "        try:\n",
        "            X=X.reshape(1,5)\n",
        "        except:\n",
        "            pass\n",
        "        #Creating Dataframe\n",
        "        X=pd.DataFrame(X)\n",
        "\n",
        "        #Assigning column names\n",
        "        X.columns = ['questionType','asin','question','answerType','category','description','title','brand','price','main_cat','feature']\n",
        "\n",
        "    elif type(X)==pd.DataFrame:\n",
        "        if verbose!=0:\n",
        "            print(\"Got Dataframe as an Input ...\")\n",
        "\n",
        "\n",
        "    #Defining model parameters\n",
        "    seq_len = [seq_lengths[0],seq_lengths[1]]\n",
        "    text_features_seqlen = [seq_lengths[2],seq_lengths[3],seq_lengths[4],seq_lengths[5]]\n",
        "    units = [128,128,128]\n",
        "\n",
        "    ques_emb_weights = np.zeros((vocab_sizes[0],150))\n",
        "    ans_emb_weights = np.zeros((vocab_sizes[1],150))\n",
        "\n",
        "    text1_emb_weights = np.zeros((text_vocab_sizes[0],150))\n",
        "    text2_emb_weights = np.zeros((text_vocab_sizes[1],150))\n",
        "    text3_emb_weights = np.zeros((text_vocab_sizes[2],150))\n",
        "    text4_emb_weights = np.zeros((text_vocab_sizes[3],150))\n",
        "\n",
        "    emb_weights = []\n",
        "\n",
        "    emb_weights = [[ques_emb_weights],[ans_emb_weights],[text1_emb_weights],\n",
        "                                                        [text2_emb_weights],\n",
        "                                                        [text3_emb_weights],\n",
        "                                                        [text4_emb_weights]]\n",
        "    if verbose!=0:\n",
        "        print(\"Loading Model ...\")\n",
        "        \n",
        "    global model\n",
        "\n",
        "    if not 'model' in globals():\n",
        "        global model\n",
        "        #Creating,calling model and Loading Weights\n",
        "        tf.keras.backend.clear_session()\n",
        "        model  = MyModel(seq_len,text_features_seqlen,vocab_sizes,text_vocab_sizes,'concat',units,emb_weights,False)\n",
        "        model.compile(optimizer='adam',loss=maskedLoss)\n",
        "        model.fit(data,answers_train, epochs=1,batch_size=1,steps_per_epoch = 1 )\n",
        "\n",
        "        #Loading pre-trained weights\n",
        "        model.load_weights(\"/content/drive/My Drive/Attention/Weights/weights-10-4.4619.hdf5\")\n",
        "\n",
        "\n",
        "        del emb_weights,ques_emb_weights,ans_emb_weights,text1_emb_weights,text2_emb_weights,text3_emb_weights,text4_emb_weights\n",
        "        del data,answers_train\n",
        "\n",
        "    try:\n",
        "        del model.encoder.embedding\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    \n",
        "\n",
        "    if type(y)==float:\n",
        "        #Predicting Answers\n",
        "        pred_answers = predict_answers(X,encoders,tokenizers,seq_lengths,fastext_models,model,verbose=verbose)\n",
        "        return pred_answers\n",
        "    else:\n",
        "        if verbose!=0:\n",
        "            print(\"Pre-Processing Data ...\")\n",
        "        seq_len = [seq_lengths[0],seq_lengths[1]]\n",
        "        text_features_seqlen = [seq_lengths[2],seq_lengths[3],seq_lengths[4],seq_lengths[5]]\n",
        "\n",
        "        processed_ques = np.array([])\n",
        "        q_mask = np.array([])\n",
        "        processed_ans = np.array([])\n",
        "\n",
        "        tf1 = np.array([])\n",
        "        tf2 = np.array([])\n",
        "        tf3 = np.array([])\n",
        "        tf4 =np.array([])\n",
        "        processed_tf= np.array([])\n",
        "        processed_of = np.array([])\n",
        "        for i in tqdm(range(len(X))):\n",
        "            #Fetching Question\n",
        "            question = X['question'].values[i]\n",
        "\n",
        "            #Fetching Answer \n",
        "            answer = y[i] \n",
        "\n",
        "            #Fetching text features\n",
        "            text_features = []\n",
        "            text_columns = ['description','title','feature','brand']\n",
        "            for column in text_columns:\n",
        "                text_features.append(X[column].values[i])\n",
        "\n",
        "            #Fetching other features\n",
        "            other_features = (X[['questionType','answerType','category','main_cat','price']].values)[i] \n",
        "\n",
        "            #Pre-processing data \n",
        "            ques_tokenizer=tokenizers[0]\n",
        "            ans_tokenizer=tokenizers[1]\n",
        "\n",
        "            ques_seqlen=seq_len[0]\n",
        "            ans_seqlen=seq_len[1]\n",
        "\n",
        "            question,ques_mask = preprocess_sentence(question,ques_seqlen,fastext_models[0])\n",
        "\n",
        "            #answer,ans_mask = preprocess_sentence(answer,ans_seqlen,fastext_models[1])\n",
        "            answer = preprocess_answer(answer , ans_seqlen ,tokenizers[1], fastext_models[1])\n",
        "            for j in range(len(text_features)):\n",
        "                text_features[j],_ = preprocess_sentence(text_features[j],text_features_seqlen[j],fastext_models[j+2],add_tokens=False)\n",
        "            \n",
        "            tf1 = np.append(tf1,text_features[0])\n",
        "            tf2 = np.append(tf2,text_features[1])\n",
        "            tf3 = np.append(tf3,text_features[2])\n",
        "            tf4 = np.append(tf4,text_features[3])\n",
        "\n",
        "            other_features = process_other_features(other_features,encoders)\n",
        "\n",
        "            processed_ques = np.append(processed_ques,question)\n",
        "            q_mask = np.append(ques_mask,q_mask)\n",
        "            processed_ans = np.append(processed_ans,answer)\n",
        "            processed_of = np.append(processed_of,other_features)\n",
        "\n",
        "        #Reshaping data\n",
        "        processed_ques = processed_ques.reshape(len(X), ques_seqlen,150)\n",
        "        q_mask = q_mask.reshape(len(X), ques_seqlen)\n",
        "        #processed_ans = processed_ans.reshape(len(X), ans_seqlen,150)\n",
        "        processed_ans = processed_ans.reshape(len(X), ans_seqlen)\n",
        "\n",
        "        processed_of = processed_of.reshape(len(X),len(other_features))\n",
        "\n",
        "        tf1 = tf1.reshape(len(X),text_features_seqlen[0],150)\n",
        "        tf2 = tf2.reshape(len(X),text_features_seqlen[1],150)\n",
        "        tf3 = tf3.reshape(len(X),text_features_seqlen[2],150)\n",
        "        tf4 = tf4.reshape(len(X),text_features_seqlen[3],150)\n",
        "        processed_tf = [tf1,tf2,tf3,tf4]\n",
        "\n",
        "        #Final data for model evaluation\n",
        "        x = [processed_ques,processed_ans,tf1,tf2,tf3,tf4,processed_of,q_mask]\n",
        "        y = processed_ans\n",
        "        if verbose!=0:\n",
        "            print(\"Calculating Metric ...\")\n",
        "        #Evaluating\n",
        "        loss = model.evaluate(x,y,verbose=1)\n",
        "\n",
        "        return calculate_perplexity(loss)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tsexi1bFb-zB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading Data\n",
        "\n",
        "import pickle\n",
        "\n",
        "#with open('/content/drive/My Drive/X_trainn.pickle', 'rb') as file:\n",
        "#    X_train = pickle.load(file)\n",
        "\n",
        "#with open('/content/drive/My Drive/answers_trainn.pickle', 'rb') as file:\n",
        "#    answers_train = pickle.load(file)\n",
        "\n",
        "\n",
        "with open('/content/drive/My Drive/data/X_test.pickle', 'rb') as file:\n",
        "    X_test = pickle.load(file)\n",
        "\n",
        "with open('/content/drive/My Drive/data/answers_test.pickle', 'rb') as file:\n",
        "    answers_test = pickle.load(file)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzRfd_Wp9kT8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7aeb45b9f0e242758f3adaba073f1aca",
            "b78bb2bb916c45d5a895e4459b9ed2c5",
            "e4dd8d57dfb0416b8b23fd2e63988adf",
            "d16dacf6227b43138f431d83ca92d1b6",
            "6951ca9081414c1b9b78059402f6b84b",
            "cfc403f9787b43beb7a2dd282478ef78",
            "6ad5f5b37160487aab7f809c347e31d8",
            "a32217008aeb4a7c8259d6b61e1e968f"
          ]
        },
        "outputId": "fef828b6-6dca-4661-ddfd-871be1b83601"
      },
      "source": [
        "X= X_test.iloc[:20].values\n",
        "y_pred = Function_1(X,verbose=2) #verbose=2 for printing questions and predicted answers."
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Got Array as an Input ...\n",
            "Loading Model ...\n",
            "Predicting Answers ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7aeb45b9f0e242758f3adaba073f1aca",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Question -->  are replacement o rings available from the manufacturer ? \n",
            "Predicted Answer -->  yes  \n",
            "\n",
            "================================================================================ \n",
            "\n",
            "Question -->  will this work with model sp5 sports model \n",
            "Predicted Answer -->  i do not know . i have a samsung galaxy and it works great .  \n",
            "\n",
            "================================================================================ \n",
            "\n",
            "Question -->  am i correct in assuming that this is manual zoom ? no power zoom ? \n",
            "Predicted Answer -->  the lens is the aperture . the aperture is the aperture .  \n",
            "\n",
            "================================================================================ \n",
            "\n",
            "Question -->  are these pads originals ? i got a pair a few months and they dont feel the same and they are falling apart . \n",
            "Predicted Answer -->  no they are not .  \n",
            "\n",
            "================================================================================ \n",
            "\n",
            "Question -->  i took the mirror apart and compared it . it is not the same . red wire is missing which is heated power and couple of needles in the plug . help ? \n",
            "Predicted Answer -->  the one i have had the same problem .  \n",
            "\n",
            "================================================================================ \n",
            "\n",
            "Question -->  this at amp t unlocked phone will work in south america ? \n",
            "Predicted Answer -->  yes , it will work with any gsm carrier .  \n",
            "\n",
            "================================================================================ \n",
            "\n",
            "Question -->  will this fit a camera body with a grip attached ? \n",
            "Predicted Answer -->  yes , it will fit .  \n",
            "\n",
            "================================================================================ \n",
            "\n",
            "Question -->  could i use it in colombia with no problem ? \n",
            "Predicted Answer -->  yes , it is a good choice .  \n",
            "\n",
            "================================================================================ \n",
            "\n",
            "Question -->  how long is the cord ? thanks . \n",
            "Predicted Answer -->  the cord is about 3 feet long .  \n",
            "\n",
            "================================================================================ \n",
            "\n",
            "Question -->  will this fit the 5c ? \n",
            "Predicted Answer -->  no  \n",
            "\n",
            "================================================================================ \n",
            "\n",
            "Question -->  i have kindle 1st and 2nd generation will this fit this ? \n",
            "Predicted Answer -->  i have a 2013 macbook pro . it is a great case .  \n",
            "\n",
            "================================================================================ \n",
            "\n",
            "Question -->  does this phone have gps navigation ? \n",
            "Predicted Answer -->  i have not tried it but i do not know .  \n",
            "\n",
            "================================================================================ \n",
            "\n",
            "Question -->  could this be use as a personal fan for my office desk ? \n",
            "Predicted Answer -->  i do not think so . i would not recommend it .  \n",
            "\n",
            "================================================================================ \n",
            "\n",
            "Question -->  in answered questions , one answer said twenty regarding quantity , another stated 4 . is this price 5 . 90 for 4 or 20 pieces ? \n",
            "Predicted Answer -->  i think they are .  \n",
            "\n",
            "================================================================================ \n",
            "\n",
            "Question -->  will it do gps like waze ? i got the stereo installed but bummed i can not run apps . . and advice is welcomed ! i have the pioneer 5600bhs \n",
            "Predicted Answer -->  yes , it is a great product .  \n",
            "\n",
            "================================================================================ \n",
            "\n",
            "Question -->  the stock inf g37 pads have 2 pieces , a shim and plate with flat surface for the piston . stock shims do not fit due to bumps , just install as is ? \n",
            "Predicted Answer -->  i have had no problems with the pads . i have had no problems with the originals . i have had no problems with the originals .  \n",
            "\n",
            "================================================================================ \n",
            "\n",
            "Question -->  will it fit an 06 durango steering wheel ? just purchased the vehicle and it is missing the ram head from the air bag cover . \n",
            "Predicted Answer -->  i have a 2013 and it fit perfectly .  \n",
            "\n",
            "================================================================================ \n",
            "\n",
            "Question -->  i am a female around 100 lbs . will i be able to use this with ease ? \n",
            "Predicted Answer -->  i do not know . i have a hefty of the hose and it works great .  \n",
            "\n",
            "================================================================================ \n",
            "\n",
            "Question -->  i am new to embroidery and have a janome mc 200e . i would like to download patterns from the internet . what do i need ? \n",
            "Predicted Answer -->  i have a janome machine and it works great . i have a janome machine and it works great .  \n",
            "\n",
            "================================================================================ \n",
            "\n",
            "Question -->  will this fit a gtp 7510 \n",
            "Predicted Answer -->  yes , it will fit .  \n",
            "\n",
            "================================================================================ \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HOv1n3NnsvA",
        "colab_type": "text"
      },
      "source": [
        "# Function 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceYvPBQxnsGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Function_2(X,y,verbose):\n",
        "    return Function_1(X,y,verbose)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNDeBMixn3vv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X= X_test.iloc[:20].values\n",
        "y = answers_test[:20]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWuaZ0cYn1c_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178,
          "referenced_widgets": [
            "f5781a293e374fa5ad05374c4a157f2f",
            "a4784e0664fc49f6b049cbb376f9f865",
            "58e5f9c9ecc84eff85bbe5c617d041e9",
            "8dd8d9fadf164af8825ddd99bf5aca44",
            "cd5cb4826b6049e4bbb868665796640c",
            "9a6cdba420474643a7d6975951bbccc8",
            "1a73118396be43eb994f5df79834af00",
            "0b4fc514a2b944309fb8455748fefd84"
          ]
        },
        "outputId": "bca3881e-a162-4e3b-d614-1b7ee7d39718"
      },
      "source": [
        "print(\"Perplexity --> \",Function_2(X,y,verbose=1))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Got Array as an Input ...\n",
            "Loading Model ...\n",
            "Pre-Processing Data ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5781a293e374fa5ad05374c4a157f2f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Calculating Metric ...\n",
            "\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.5087\n",
            "Perplexity -->  90.799484\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}